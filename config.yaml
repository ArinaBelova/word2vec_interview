data:
  # Options: "file" (use file_path), "brown", "reuters", "gutenberg", "combined", "text8", "wikitext-2", "wikitext-103"
  source: "wikitext-103" #"brown"
  file_path: "data.txt"  # Only used if source is "file" for very dummy experimentation
  max_sentences: null    # Limit sentences (null for all)

model:
  embedding_dim: 300 # in the paper: 300, can play around with it
  window_size: 5 # in the paper: 5, can try 15 to increase the performance
  negative_samples: 10 # 5-20 for small datasets, 2-5 for large datasets
  subsample_threshold: 0.00001  # 10e-5 eas in the paper Threshold for subsampling frequent words
  batch_size: 4096            # Larger batches for Numba parallel processing
  n_threads: 20              # Number of Numba parallel threads
  save_model_path: "word2vec_model_larger_embedding_wikitext.pkl"  # Path to save the trained model
  save_embeddings_path: "embeddings_large_wikitext.txt"  # Path to save the embeddings in text format

training:
  epochs: 100
  learning_rate: 0.001 #0.025
  log_interval: 5

wandb:
  project: "word2vec"
  entity: null
  run_name: null